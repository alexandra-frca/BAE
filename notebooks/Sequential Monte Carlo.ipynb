{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and auxiliary functions"
      ],
      "metadata": {
        "id": "-ksuGMAkqpJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "rng = np.random.default_rng()\n",
        "from scipy.stats import truncnorm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def print_results(mean, std, info):\n",
        "    # Use 2 significant digits unless standard deviation is zero.\n",
        "    digits = 10 if std==0 else int(np.ceil(-np.log10(abs(std))))+1\n",
        "    print(f\"   *{info} result: theta = \"\n",
        "          f\"{myround(mean,digits)} Â± {myround(std,digits)}\")\n",
        "\n",
        "def myround(x, Ndig):\n",
        "    '''\n",
        "    To do {:.nf}.format() for arbitrary n. It works like 'round', but\n",
        "    perserves zeros when printing, i.e. format(1.000,2) = 1.00 while\n",
        "    round(1.000,2) = 1.0.\n",
        "    '''\n",
        "    xf = f\"{{:.{Ndig}f}}\".format(x)\n",
        "    return xf\n",
        "\n",
        "def get_truncated_normal(mean, sd, low=0, upp=10):\n",
        "    '''\n",
        "      Because Scipy truncnorm's arguments are impractical.\n",
        "    '''\n",
        "    return truncnorm(\n",
        "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)"
      ],
      "metadata": {
        "id": "d6dJffLAqHcf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SMC base class"
      ],
      "metadata": {
        "id": "lgHx7TyTjfBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MemWAo3YiuNG"
      },
      "outputs": [],
      "source": [
        "class SMCsampler():\n",
        "    def __init__(self, Npart: int, model, thr = 0.5):\n",
        "\n",
        "        self.Npart = Npart # Number of particles.\n",
        "        self.model = model # Generative model.\n",
        "        self.thr = thr # Resampling threshold (fraction of ESS).\n",
        "        self.str = None # ID for the resampler (RWM or LW).\n",
        "\n",
        "        # Init sampler to a flat prior on [0, scale].\n",
        "        self.locs = np.random.uniform(size = Npart) # Particle locations.\n",
        "        self.locs *= model.scale # Domain upper bound (lower is assumed 0).\n",
        "        self.weights = np.ones(Npart)/Npart # Particle weights (here uniform).\n",
        "\n",
        "        # Keep track of the sum of weights so it's easier to normalize.\n",
        "        self.wsum = 1\n",
        "\n",
        "    def offline_inference(self, data):\n",
        "        '''\n",
        "        Use SMC to sample from an incremental dataset containing 1 datum, then\n",
        "        2 data, ..., then all data. Despite the inference being offline, we want\n",
        "        full granularity to allow for resampling when needed.\n",
        "\n",
        "        For the weight updates, only the last datum, which should be the latest\n",
        "        at the current stage, is used. For resampling, the entire dataset may be\n",
        "        used, and needs to if we are to fully preserve the distribution.\n",
        "        '''\n",
        "        for i in range(1, len(data)+1):\n",
        "            self.update(data.partial_data(i))\n",
        "\n",
        "        results = self.mean_and_std()\n",
        "        print_results(*results, f\"{self.str.ljust(3)}\")\n",
        "\n",
        "    def update(self, data):\n",
        "        '''\n",
        "        Update distribution based on the latest datum.\n",
        "\n",
        "        It is assumed the data arrive sequentially, 'data' being the latest\n",
        "        available dataset (chronologically ordered).\n",
        "        '''\n",
        "        # Get the latest datum for the weight updates.\n",
        "        ctrl, outcome = data.ctrls[-1], data.outcomes[-1]\n",
        "\n",
        "        # Get the likelihoods associated with each particle.\n",
        "        likelihood = self.model.likelihood\n",
        "        ls = likelihood(self.locs, ctrl, outcome)\n",
        "\n",
        "        # Reweight.\n",
        "        self.weights = self.weights*ls\n",
        "\n",
        "        # Calculate effective sample size.\n",
        "        self.wsum = np.sum(self.weights)\n",
        "        wsum2 = np.sum(self.weights**2)\n",
        "        ESS = self.wsum**2/wsum2\n",
        "\n",
        "        # Resample if the weights are dominated by too small a fraction of\n",
        "        # the particles (statistical power is critically low).\n",
        "        ESS_thr = self.thr*self.Npart\n",
        "        if ESS < ESS_thr:\n",
        "            self.resample(data)\n",
        "\n",
        "    def resample(self, data):\n",
        "        '''\n",
        "        Change particle locations to introduce variability. The mechanism may\n",
        "        vary. There is a trade-off between statisticall correctness, processing\n",
        "        cost, and space exploration.\n",
        "        '''\n",
        "        # Multinomial sampling with replacement from the current particle cloud.\n",
        "        new_locs = rng.choice(self.locs, size=self.Npart,\n",
        "                              p=self.weights/self.wsum)\n",
        "\n",
        "        # Introduce variability by resampling.\n",
        "        new_locs = self.resampler(new_locs, data)\n",
        "\n",
        "        # After resampling the distribution is uniform.\n",
        "        self.locs = new_locs\n",
        "        self.weights = np.ones(self.Npart)/self.Npart\n",
        "\n",
        "         # Update norm.\n",
        "        self.wsum = 1\n",
        "\n",
        "    def mean_and_std(self):\n",
        "        mean = self.mean()\n",
        "        var = np.sum((self.locs-mean)**2*self.weights)/self.wsum\n",
        "        return mean, np.sqrt(var)\n",
        "\n",
        "    def mean(self):\n",
        "        mean = np.sum(self.locs*self.weights)/self.wsum\n",
        "        return mean"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Liu-West\n",
        "\n",
        "The resampling mechanism is the Liu-West filter (LW).\n",
        "\n",
        "I use a truncated Gaussian to ensure the proposals don't fall out of bounds."
      ],
      "metadata": {
        "id": "Hvb05mAVjmlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LiuWestSampler(SMCsampler):\n",
        "\n",
        "    def __init__(self, Npart: int, model, a = 0.98, **kwargs):\n",
        "        super().__init__(Npart, model, **kwargs)\n",
        "        self.a = a # Resampling factor.\n",
        "        self.str = \"LW\"\n",
        "\n",
        "    def resampler(self, old_locs, data):\n",
        "        # The data are not used but we need them in the signature.\n",
        "        currmean, currstd = self.mean_and_std()\n",
        "        a = self.a\n",
        "        means = a*old_locs+(1-a)*currmean\n",
        "        h = (1-a**2)**0.5\n",
        "        std = h*currstd\n",
        "\n",
        "        Tnorm = get_truncated_normal(means, std, low=0, upp=self.model.scale)\n",
        "        new_locs = Tnorm.rvs()\n",
        "        return new_locs"
      ],
      "metadata": {
        "id": "WwVPmVvnjY35"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metropolis (RWM)\n",
        "\n",
        "The resampling mechanism is random walk Metropolis (RWM) with samples drawn from a Gaussian.\n",
        "\n",
        "The variance of the Gaussian is a free parameter that strongly affects the performance. There's a trade-off involved in assuring proper space exploration: the proposals should be bold enough that they cover enough ground, but conservative enough that their acceptance probabilities are not too low. The effectiveness can be assessed by the acceptance probabilities.\n",
        "\n",
        "In MCMC, this parameter may be chosen in a warm up phase. In SMC, we don't expect a fixed parameter to do well for the entire execution, as the distribution shape changes drastically through it. On the other hand, and unusually, we have access to handy approximate statistics of the distribution we are sampling from.\n",
        "\n",
        "I choose the proposal variance to be proportional to the variance of the current SMC distribution, so that the proposals become closer to the original particles as the distribution becomes sharper. In the beginning, where the distribution they're targeting is flatter, the proposals can (should) be more distant and still have a fair chance of acceptance.\n",
        "\n",
        "The proportionality coeffiecient can be chosen heuristically; here I used 1."
      ],
      "metadata": {
        "id": "-gPJ6_1Ajoq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetropolisSampler(SMCsampler):\n",
        "    def __init__(self, Npart: int, model, factor=1.5, **kwargs):\n",
        "        super().__init__(Npart, model, **kwargs)\n",
        "        self.factor = factor\n",
        "        self.str = \"RWM\"\n",
        "\n",
        "    def resampler(self, old_locs, data):\n",
        "        # Propose new locations.\n",
        "        proposals = self.generate_proposals(old_locs, data)\n",
        "        # Calculate acceptance rates.\n",
        "        acc_rates = self.acceptance_rates(proposals, old_locs, data)\n",
        "        # Probabilistic rejection step.\n",
        "        new_locs = self.rejection_step(proposals, acc_rates, old_locs)\n",
        "        return new_locs\n",
        "\n",
        "    def generate_proposals(self, old_locs, data):\n",
        "        '''\n",
        "        Propose new locations via a Gaussian perturbation of the old ones.\n",
        "\n",
        "        The variance of the Gaussian is chosen to be proportional to that of the\n",
        "        current SMC distribution, which approximates the target distribution.\n",
        "        '''\n",
        "        # Get the parameter for the proposal distribution.\n",
        "        _, currsd = self.mean_and_std()\n",
        "        sd = self.factor*currsd\n",
        "\n",
        "        # Get the samples.\n",
        "        Tnorm = get_truncated_normal(old_locs, sd, low=0, upp=self.model.scale)\n",
        "        proposals = Tnorm.rvs()\n",
        "        return proposals\n",
        "\n",
        "    def acceptance_rates(self, proposals, old_locs, data):\n",
        "        '''\n",
        "        Calculate the Metropolis acceptance rates that assure detailed balance.\n",
        "        '''\n",
        "        old_likelihoods = self.model.batch_likelihood(old_locs, data)\n",
        "        new_likelihoods = self.model.batch_likelihood(proposals, data)\n",
        "        acc_rates = np.where(old_likelihoods>0, new_likelihoods/old_likelihoods,\n",
        "                             1)\n",
        "        acc_rates = self.filter_acc_rates(acc_rates, new_likelihoods,\n",
        "                                          old_likelihoods)\n",
        "        return acc_rates\n",
        "\n",
        "    @staticmethod\n",
        "    def filter_acc_rates(acc_rates, new_likelihoods, old_likelihoods):\n",
        "        '''\n",
        "        Some calculated acceptance rates may not be valid probabilities.\n",
        "        This function:\n",
        "        * Caps > 1 values at 1:  Metropolis rate should be min(1, Pnew/Pold).\n",
        "        * Fixes NaNs created by division by zero: when Pold=0, we just accept\n",
        "        the new proposal.\n",
        "\n",
        "        '''\n",
        "        # Not the clearest way but the fastest.\n",
        "        acc_rates = np.where(np.logical_or(old_likelihoods==0, acc_rates>1),\n",
        "                      np.ceil(new_likelihoods), acc_rates)\n",
        "        return acc_rates\n",
        "\n",
        "    @staticmethod\n",
        "    def rejection_step(proposals, acc_rates, old_locs):\n",
        "        '''\n",
        "        Probabilistically accept or reject the new samples.\n",
        "        '''\n",
        "        accept = np.random.binomial(1, acc_rates)\n",
        "        new_locs = np.where(accept, proposals, old_locs)\n",
        "        return new_locs"
      ],
      "metadata": {
        "id": "SM7hVQwejb22"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "To store information from a sequence of single shot binary measurements with one experimental control."
      ],
      "metadata": {
        "id": "VPOfPUDZaDEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MeasurementData:\n",
        "    ctrls: [float]\n",
        "    outcomes: [int]\n",
        "\n",
        "    def partial_data(self, Ndata):\n",
        "      if Ndata > len(self):\n",
        "          print(\"> The requested partial dataset is exceeds the available \"\n",
        "                \"dataset's length.\")\n",
        "      return MeasurementData(self.ctrls[:Ndata], self.outcomes[:Ndata])\n",
        "\n",
        "    def __len__(self):\n",
        "      l1 = len(self.ctrls)\n",
        "      l2 = len(self.outcomes)\n",
        "      if l1 != l2:\n",
        "          print(\"> The length of the dataset controls and outcomes is unmatched.\")\n",
        "          return -1\n",
        "      else:\n",
        "          return l1\n",
        "\n",
        "    def __str__(self):\n",
        "      s = \"MeasurementData instance:\\n\"\n",
        "      s += f\"* {len(self.ctrls)} controls: \" + str(self.ctrls)\n",
        "      s += f\"\\n* {len(self.ctrls)} outcomes: \" + str(self.outcomes)\n",
        "      return s"
      ],
      "metadata": {
        "id": "ogNfrlcdmwUN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "A simple test model for binary measurements $x \\in \\{0,1\\}$ with $P(1 \\mid \\theta; t) = \\sin^2(\\theta \\cdot t)$, where $\\theta \\in ]0, \\pi/2]$ is a model parameter and $t$ an  experimental control.\n",
        "\n",
        "This will be used to generate data for the inference.\n",
        "\n",
        "To show a scenario where Liu West fails, I made up an alternative way of generating trickier data, where we introduce a temporary decoy parameter. For the first half of the measurements, we alternate between measurements based on the true parameter $\\theta_{real}$ and measurements based on a decoy parameter $\\theta_{decoy}$. I.e., we sample from:\n",
        "\n",
        "$$\\frac{P(x \\mid \\theta_{real}; t)}{2} + \\frac{P(x \\mid \\theta_{decoy}; t)}{2}$$\n",
        "\n",
        "This will introduce bimodality in the posterior, with a mode at $\\theta_{real}$ and another at $\\theta_{decoy}$.\n",
        "\n",
        "After these measurements, we eliminate the decoy and start drawing samples exclusively from the true distribution. This will resolve the ambiguity, as the probability of $\\theta_{decoy}$ will drop.\n",
        "\n",
        "We expect that this will throw off Liu-West, because its unimodality assumption will drag the particles to the midpoint between these modes. After the decoy phase is over, it will not be able to recover, because there will be particle depletion in the correct region of space.\n",
        "\n",
        "It is clear Liu-West cannot salvage such a misled sampler, because it doesn't use any information not already contained in it. On the contrary: it does not even use all of the available information.\n",
        "\n",
        "On the other hand, with Metropolis we expect that an equitative fraction of particles will gather around each mode. When the redundance is lifted, the ones covering the decoy mode should join the others around the true value. Even if that were not the case, in theory, a good Markov kernel could correct the problem, because it converges to the real distribution rather than the SMC approximation (or even a simplification thereof, as with LW).\n",
        "\n",
        "We choose $\\theta_{decoy} = \\theta_{real} \\pm 1$, with whichever sign makes it fall the correct domain. This is just an arbitrary option that seems to illustrate the point well for various values of $\\theta_{real}$."
      ],
      "metadata": {
        "id": "rb8M5TmYjwRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SineModel():\n",
        "    def __init__(self, theta, scale = np.pi/2 ):\n",
        "        self.theta = theta\n",
        "\n",
        "        # Upper bound to the parameter value. Lower is assumed 0.\n",
        "        self.scale = scale\n",
        "\n",
        "    def measure(self, t):\n",
        "        p1 = np.sin(self.theta*t)**2\n",
        "        outcome = np.random.binomial(1, p1)\n",
        "        return outcome\n",
        "\n",
        "    def get_data(self, ts, tricky = False):\n",
        "        if tricky:\n",
        "            return self.get_tricky_data(ts)\n",
        "\n",
        "        xs = self.measure(ts)\n",
        "        data = MeasurementData(ts, xs)\n",
        "        return data\n",
        "\n",
        "    def get_tricky_data(self, ts, bfrac = 0.5):\n",
        "        '''\n",
        "        Data that is initially redundant. The measurements for the first 'bfrac'\n",
        "        (fractional) times sample from a bimodal distribution. Unimodality holds\n",
        "        for the rest (at least asymptotically).\n",
        "        '''\n",
        "        assert bfrac > 0 and bfrac < 1\n",
        "\n",
        "        Nmeas = len(ts)\n",
        "        # Times where the binomial parameter will change between true and decoy.\n",
        "        ts1 = ts[:int(bfrac*Nmeas)]\n",
        "        xs1 = self.measure_decoy(ts1)\n",
        "\n",
        "        # Times where the binomial parameter will be fixed (true only).\n",
        "        ts2 = ts[int(bfrac*Nmeas):]\n",
        "        xs2 = self.measure(ts2)\n",
        "\n",
        "        # Combine the data from the bimodal and unimodal phases, by this order.\n",
        "        xs = np.concatenate((xs1, xs2))\n",
        "\n",
        "        data = MeasurementData(ts, xs)\n",
        "        return data\n",
        "\n",
        "    def measure_decoy(self, ts):\n",
        "        '''\n",
        "        Sample measurement outcomes from a bimodal distribution where one of\n",
        "        the modes is the true parameter and the other is a decoy.\n",
        "\n",
        "        The used parameter alternates between the two options (decoy or true)\n",
        "        through 'ts'.\n",
        "        '''\n",
        "        # Save original parameter.\n",
        "        real_theta = self.theta\n",
        "\n",
        "        # Set decoy parameter and make measurements accordingly, for even times.\n",
        "        decoy = self.theta - 1 if self.theta > 1 else self.theta + 1\n",
        "        self.theta = decoy\n",
        "        xs1 = self.measure(ts[0::2])\n",
        "\n",
        "        # Reset true parameter and make measurements accordingly, for odd times.\n",
        "        self.theta = real_theta\n",
        "        xs2 = self.measure(ts[1::2])\n",
        "\n",
        "        # Intercalate elements of each outcome list to make a single array.\n",
        "        xsboth = np.dstack((xs1,xs2)).flatten()\n",
        "        return xsboth\n",
        "\n",
        "    def likelihood(self, thetas, ts, outcomes):\n",
        "        # Calculate P(1|theta; t).\n",
        "        p1 = np.sin(thetas*ts)**2\n",
        "\n",
        "        # Correct for outcome 0.\n",
        "        ls = p1 if outcomes==1 else 1-p1\n",
        "\n",
        "        # Ensure theta is in the correct domain, i.e. [0, scale].\n",
        "        ls = self.enforce_domain(thetas, ls)\n",
        "        return ls\n",
        "\n",
        "    def enforce_domain(self, thetas, ls):\n",
        "        '''\n",
        "        Correct the likelihood of out-of-bounds parameters. Parameters not in\n",
        "        [0, scale] should have 0 likelihood.\n",
        "        '''\n",
        "        valid = np.logical_and(thetas >= 0, thetas <= self.scale)\n",
        "        ls = np.where(valid, ls, 0)\n",
        "        return ls\n",
        "\n",
        "    def batch_likelihood(self, thetas, data):\n",
        "        '''\n",
        "        Calculate the likelihood based on a dataset, rather than a single datum.\n",
        "        '''\n",
        "        ctrls, xs = np.array(data.ctrls), np.array(data.outcomes)\n",
        "\n",
        "        args = np.outer(thetas, ctrls)\n",
        "        sin2 = np.sin(args)**2\n",
        "        Ls_joint = np.prod(sin2**xs*(1-sin2)**(1-xs), axis = 1)\n",
        "\n",
        "        Ls_joint = self.enforce_domain(thetas, Ls_joint)\n",
        "        return Ls_joint"
      ],
      "metadata": {
        "id": "WmYJrQlMjx88"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests\n",
        "\n",
        "Test Liu-West and Metropolis; first using simple data from the sine model, then using the tricky data."
      ],
      "metadata": {
        "id": "D32IvPXtjyW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_normal_and_tricky(theta, params):\n",
        "    print(f\"> Testing inference for theta = {theta}.\")\n",
        "\n",
        "    # Test inference on the basic sin^2 data.\n",
        "    print(\"\\n1. Straightforward data.\")\n",
        "    test_both_samplers(**params)\n",
        "    params[\"tricky\"] = True\n",
        "\n",
        "    # Test inference on the tricky sin^2 data.\n",
        "    print(f\"\\n2. Tricky data (initially bimodal).\")\n",
        "    test_both_samplers(**params)\n",
        "\n",
        "def test_both_samplers(**args):\n",
        "    '''\n",
        "    Test the performance of Liu West and Metropolis.\n",
        "    '''\n",
        "    for which in [\"LW\", \"RWM\"]:\n",
        "        test(which, **args)\n",
        "\n",
        "def test(which, theta, tmax, Nmeas, tricky, Npart, thr, a = None, factor = None):\n",
        "    '''\n",
        "    Test the performance of Liu West or Metropolis.\n",
        "    '''\n",
        "    # Create model and generate data for the inference.\n",
        "    model = SineModel(theta)\n",
        "    ts = np.linspace(0, tmax, Nmeas)\n",
        "    data = model.get_data(ts, tricky = tricky)\n",
        "\n",
        "    # Common sampler arguments.\n",
        "    cargs = {\"Npart\": Npart, \"model\": model, \"thr\": thr}\n",
        "\n",
        "    # Get requested sampler.\n",
        "    if which==\"LW\":\n",
        "        sampler = LiuWestSampler(**cargs, a = a)\n",
        "    if which==\"RWM\":\n",
        "        sampler = MetropolisSampler(**cargs, factor = factor)\n",
        "\n",
        "    # Perform inference on the dataset.\n",
        "    sampler.offline_inference(data)"
      ],
      "metadata": {
        "id": "oJ3skbPCjzt3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True parameter.\n",
        "theta = 1.2\n",
        "\n",
        "# Problem and sampler parameters.\n",
        "params = {\"theta\": theta,\n",
        "          \"tricky\": False,\n",
        "          \"tmax\": 50,\n",
        "          \"Nmeas\": 300,\n",
        "          \"Npart\": 5000,\n",
        "          \"thr\": 0.5,\n",
        "          \"a\": 0.98,\n",
        "          \"factor\": 1}\n",
        "\n",
        "test_normal_and_tricky(theta, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EsSNaCnSN7e",
        "outputId": "c616f5ac-f09f-4e2e-eb1d-e0a4f88de54c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Testing inference for theta = 1.2.\n",
            "\n",
            "1. Straightforward data.\n",
            "   *LW  result: theta = 1.20104 Â± 0.00094\n",
            "   *RWM result: theta = 1.1990 Â± 0.0011\n",
            "\n",
            "2. Tricky data (initially bimodal).\n",
            "   *LW  result: theta = 0.97 Â± 0.15\n",
            "   *RWM result: theta = 0.18378 Â± 0.00082\n"
          ]
        }
      ]
    }
  ]
}